{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b85a12-fdf8-4497-a24b-ed189f80f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import OperationalError\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c9bd92-8b66-436b-ac8a-8f10988fe959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versions\n",
    "# Python => 3.11.7 # Pandas => 2.1.4 # hdbcli => 2.20.15 # psycopg => 3.1.18 # sqlalchemy.2.0.25 # sqlalchemy-hdbcli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf074a9-c5f5-4d42-a989-31d62a51efcf",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9684f1e5-ba32-4134-b2c4-94e44ffa36aa",
   "metadata": {},
   "source": [
    "<ul>\n",
    "        <li>1. Database Connection Functions\n",
    "            <ul>\n",
    "                <li>1.1. Function - SAP Connection</li>\n",
    "                <li>1.2. Function - PostgreSQL Connection</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>2. Extract Data from SAP\n",
    "            <ul>\n",
    "                <li>2.1. Function - Extract from SAP</li>\n",
    "                <li>2.2. Set up Staging File Paths</li>\n",
    "                <li>2.3. Define SQL Queries to Extract Data from SAP</li>\n",
    "                <li>2.4. Run Extract SQL Queries to Extract Data from SAP</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>3. Load Data into PostgreSQL\n",
    "            <ul>\n",
    "                <li>3.1. Function - Insert Dimension Tables into PostgreSQL</li>\n",
    "                <li>3.2. Function - Insert Fact Table into PostgreSQL</li>\n",
    "                <li>3.3. Create a Composite Key Column to Detect Duplicate Rows in AR</li>\n",
    "                <li>3.4. Load Dimension Tables and Fact Table into PostgreSQL</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ff9d7f-0b3e-45fc-b335-6794ad2156aa",
   "metadata": {},
   "source": [
    "## 1. Database Connection Functions ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f6a75-3e0f-40ae-9959-d3f397e38a3e",
   "metadata": {},
   "source": [
    "### 1.1. Function - SAP connection ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94291dfc-7b64-4c2a-952d-d49962698585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_sap():\n",
    "    \"\"\"Used to connect to SAP Database\"\"\"\n",
    "    \n",
    "    # Read SAP connection parameters from text file\n",
    "    with open(\"D:\\\\Data Warehousing\\\\AR HWM\\\\Credentials\\\\sap_credentials.txt\", \"r\") as file:\n",
    "        lines_sap = [line.strip() for line in file.readlines()]\n",
    "\n",
    "    host, port, user, password = lines_sap[0], int(lines_sap[1]), lines_sap[2], lines_sap[3]\n",
    "    escaped_password = urllib.parse.quote_plus(password) # Escape special characters such as @, #, $ in the password\n",
    "    \n",
    "    # Create connection string to use with sqlalchemy engine\n",
    "    sap_connection_string = \"hana+hdbcli://{}:{}@{}:{}/?encrypt=encrypt=true&sslValidateCertificate=false\".format(user, escaped_password, host, port)\n",
    "\n",
    "    # Create SQLAlchemy engine as connection\n",
    "    connection = create_engine(sap_connection_string)\n",
    "    # Check if the connection is successful\n",
    "    try:\n",
    "        connection.connect()\n",
    "        print(\"SAP Connection successful!\")\n",
    "    except OperationalError as e:\n",
    "        print(\"Error connecting to SAP database:\", e)\n",
    "\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c9bf8b-8b21-4499-96f5-078a50c110fe",
   "metadata": {},
   "source": [
    "### 1.2. Function - PostgreSQL Connection ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18963424-baaa-41af-9e4f-ae18075cd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_pgsql():\n",
    "    \"\"\"Used to connect to PostgreSQL database\"\"\"\n",
    "    \n",
    "    # Read PostgreSQL connection parameters from text\n",
    "    with open(\"D:\\\\Data Warehousing\\\\AR HWM\\\\Credentials\\\\pgsql_credentials.txt\", \"r\") as file:\n",
    "        lines_pgsql = [line.strip() for line in file.readlines()]\n",
    "\n",
    "    host, port, database, username, password = lines_pgsql[0], lines_pgsql[1], lines_pgsql[2], lines_pgsql[3], lines_pgsql[4]\n",
    "\n",
    "    # Establish PostgreSQL connection\n",
    "    try:\n",
    "        connection = psycopg2.connect(database=database,\n",
    "                                     host=host,\n",
    "                                     port=port,\n",
    "                                     user=username,\n",
    "                                     password=password)\n",
    "        print(\"PostgreSQL Connection successful!\")\n",
    "    except psycopg2.Error as e:\n",
    "        print(\"PostgreSQL Connection failed!\", e)\n",
    "\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f53ad22-2e12-4400-997d-c0748c283f17",
   "metadata": {},
   "source": [
    "## 2. Extract data from SAP ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165af08-8d1d-4ffe-ad13-09a3af2eea2e",
   "metadata": {},
   "source": [
    "#### 2.1. Function - Extract from SAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb42cf0-3eb5-46c1-9b16-78167e994664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_sap(connection, extract_query, output_folder, output_filename):\n",
    "    \"\"\"\n",
    "        Extracts data from SAP using a specified SQL query and saves the result as a CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Default folder path for sql files\n",
    "    sql_folder_path = \"D:\\\\Data Warehousing\\\\AR HWM\\\\SQL\\\\\"\n",
    "    \n",
    "    # Define the completed sql file path\n",
    "    sql_file_path = os.path.join(sql_folder_path, extract_query)\n",
    "    \n",
    "    # Read query and extract data\n",
    "    with open(sql_file_path, \"r\") as file:\n",
    "        extract_item_query = file.read()\n",
    "    df = pd.read_sql_query(extract_item_query, connection)\n",
    "\n",
    "    # Solve FNRC code missing issues in Customer\n",
    "    if 'u_fnrc' in df.columns:\n",
    "        df['u_fnrc'] = df['u_fnrc'].fillna(0).astype(int) # fnrc code missing issue in customer table\n",
    "\n",
    "    # Check ARCM query and transform data\n",
    "    if extract_query == \"Extract_F_ARCM_HWM.sql\" or extract_query == \"Extract_F_ARLM_HWM.sql\":\n",
    "        \n",
    "        df = df[[\"CustomerCode\", \"SellingPriceList\", \"SeriesName\", \"Document Entry\", \"Document Number\", \n",
    "                 \"LineNum\", \"Sale Employee Code\", \"PostingDate\", \"DueDate\", \"DocumentDate\", \"InvoiceType\", \n",
    "                 \"ItemCode\", \"Quantity\", \"Status\", \"Price\", \"LineTotal\", \"Document Discount\", \"DiscountSum\", \n",
    "                 \"RowDiscountPercentage\", \"Document Total\", \"Warehouse Code\", \"BPBranch\", \"Branch\", \"Department\", \n",
    "                 \"AgentCode\", \"AgentCommissionAmount\", \"Project\", \"Currency\", \"Gross Profit\", \"Line Discount total\", \n",
    "                 \"GrossTotal\", \"Stock Price\", \"Stock Value\", \"Item Last Sale Price\", \"Gross Profit(Item Cost)\", \n",
    "                 \"TotalBasePrice(LPP)\", \"TotalGrossProfit(LPP)\", \"GLAccount Code\", \"CogsAccount Code\"]] ## Get required columns\n",
    "    \n",
    "        # Rename Branch as SaleType\n",
    "        df = df.rename(columns={\"Branch\":\"SaleType\"}) # Mismatched column name in ARCM\n",
    "    \n",
    "    # Define output file path\n",
    "    output_file_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "    # Export as CSV file\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Data extracted successfully and saved to: {output_file_path} at {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298fdac-9b10-4eef-9ccd-485e859d45e8",
   "metadata": {},
   "source": [
    "#### 2.2. Set up Staging file paths ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "313f04e1-2908-4c36-a592-3c55b0097aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths for Initial Stage (csv files output)\n",
    "dim_folder_path = \"D:\\\\Data Warehousing\\\\AR HWM\\\\Dim\\\\\"\n",
    "fact_folder_path = \"D:\\\\Data Warehousing\\\\AR HWM\\\\Fact\\\\\"\n",
    "#sql_folder_path = \"D:\\\\Data Warehousing\\\\AR HWM\\\\SQL\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50360a1-cede-4544-bfd5-915155922b78",
   "metadata": {},
   "source": [
    "#### 2.3. Define SQL queries to extract data from SAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52789460-bd89-4bc8-a6a9-534d759c08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Extract Queries (SAP)\n",
    "# Dim\n",
    "extract_agent_query = \"Extract_D_Agent_HWM.sql\"\n",
    "extract_branch_query = \"Extract_D_Branch_HWM.sql\"\n",
    "extract_cogsaccount_query = \"Extract_D_COGSAccount_HWM.sql\"\n",
    "extract_customer_query = \"Extract_D_Customer_HWM.sql\"\n",
    "extract_department_query = \"Extract_D_Department_HWM.sql\"\n",
    "extract_glaccount_query = \"Extract_D_GLAccount_HWM.sql\"\n",
    "extract_item_query = \"Extract_D_Item_HWM.sql\"\n",
    "extract_saleemployee_query = \"Extract_D_SaleEmployee_HWM.sql\"\n",
    "extract_saletype_query = \"Extract_D_SaleType_HWM.sql\"\n",
    "extract_warehouse_query = \"Extract_D_Warehouse_HWM.sql\"\n",
    "# Fact\n",
    "extract_arcm_query = \"Extract_F_ARCM_HWM.sql\"\n",
    "extract_arlm_query = \"Extract_F_ARLM_HWM.sql\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6675d1c8-8995-4ce0-b369-d2145127a5e6",
   "metadata": {},
   "source": [
    "#### 2.4. Run extract SQL quereis to extract data from SAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3133d60-d138-415c-ae33-d137288529d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAP Connection successful!\n",
      "Data extracted successfully and saved to: D:\\Data Warehousing\\AR HWM\\Dim\\D_Agent.csv at 2024-06-06 10:28:38.718963\n",
      "Data extracted successfully and saved to: D:\\Data Warehousing\\AR HWM\\Dim\\D_Branch.csv at 2024-06-06 10:28:38.732973\n",
      "Data extracted successfully and saved to: D:\\Data Warehousing\\AR HWM\\Dim\\D_COGSAccount.csv at 2024-06-06 10:28:38.749965\n",
      "Data extracted successfully and saved to: D:\\Data Warehousing\\AR HWM\\Dim\\D_Customer.csv at 2024-06-06 10:28:38.820289\n",
      "Data extracted successfully and saved to: D:\\Data Warehousing\\AR HWM\\Dim\\D_Department.csv at 2024-06-06 10:28:38.835916\n",
      "Data extracted successfully and saved to: D:\\Data Warehousing\\AR HWM\\Dim\\D_GLAccount.csv at 2024-06-06 10:28:38.861717\n",
      "Data extracted successfully and saved to: D:\\Data Warehousing\\AR HWM\\Dim\\D_Item.csv at 2024-06-06 10:28:39.103053\n",
      "Data extracted successfully and saved to: D:\\Data Warehousing\\AR HWM\\Dim\\D_SaleType.csv at 2024-06-06 10:28:39.119053\n",
      "Data extracted successfully and saved to: D:\\Data Warehousing\\AR HWM\\Dim\\D_Warehouse.csv at 2024-06-06 10:28:39.135054\n",
      "Data extracted successfully and saved to: D:\\Data Warehousing\\AR HWM\\Fact\\F_AR_CM.csv at 2024-06-06 10:28:39.804639\n",
      "Data extracted successfully and saved to: D:\\Data Warehousing\\AR HWM\\Fact\\F_AR_LM.csv at 2024-06-06 10:28:41.405155\n"
     ]
    }
   ],
   "source": [
    "# Establish SAP Database connection\n",
    "conn_sap = connect_to_sap()\n",
    "try:\n",
    "    # Example calls to extract_from_sap function\n",
    "    # Dim Tables\n",
    "    extract_from_sap(conn_sap, extract_agent_query, dim_folder_path, \"D_Agent.csv\")\n",
    "    extract_from_sap(conn_sap, extract_branch_query, dim_folder_path, \"D_Branch.csv\")\n",
    "    extract_from_sap(conn_sap, extract_cogsaccount_query, dim_folder_path, \"D_COGSAccount.csv\")\n",
    "    extract_from_sap(conn_sap, extract_customer_query, dim_folder_path, \"D_Customer.csv\")\n",
    "    extract_from_sap(conn_sap, extract_department_query, dim_folder_path, \"D_Department.csv\")\n",
    "    extract_from_sap(conn_sap, extract_glaccount_query, dim_folder_path, \"D_GLAccount.csv\")\n",
    "    extract_from_sap(conn_sap, extract_item_query, dim_folder_path, \"D_Item.csv\")\n",
    "    extract_from_sap(conn_sap, extract_saletype_query, dim_folder_path, \"D_SaleType.csv\")\n",
    "    extract_from_sap(conn_sap, extract_warehouse_query, dim_folder_path, \"D_Warehouse.csv\")\n",
    "    # Fact Table (Current Month, Last Month)\n",
    "    extract_from_sap(conn_sap, extract_arcm_query, fact_folder_path, \"F_AR_CM.csv\")\n",
    "    extract_from_sap(conn_sap, extract_arlm_query, fact_folder_path, \"F_AR_LM.csv\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred during extraction:\", e)\n",
    "\n",
    "finally:\n",
    "    # Close database connection\n",
    "    conn_sap.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4f81a-50d6-419d-a7eb-d751726bd67e",
   "metadata": {},
   "source": [
    "## 3. Load data into PostgreSQL ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae4c269-50b0-4669-9339-3f21871c1134",
   "metadata": {},
   "source": [
    "#### 3.1. Function - Insert Dimension Tables into PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a576545-5100-4f1a-af90-bc29713c60ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_dim_into_pgsql(table_name):\n",
    "    \"\"\"Insert dimension data from csv files into PostgreSQL\"\"\"\n",
    "\n",
    "    # Define the DELETE command to delete old data in dimensional tables\n",
    "    delete_data_sql = f'DELETE FROM \"AR_HWM\".\"{table_name}\";'\n",
    "\n",
    "    # Define the COPY command to insert new data from csv files into dimensional tables\n",
    "    copy_sql = f'COPY \"AR_HWM\".\"{table_name}\" FROM stdin WITH CSV HEADER;'\n",
    "    \n",
    "    # Define the CSV filepath and filename to get data from this file\n",
    "    file_path = \"D:\\\\Data Warehousing\\\\AR HWM\\\\Dim\\\\\" + table_name + \".csv\"\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # Execute the DELETE command to delete old data\n",
    "        cursor_pgsql.execute(delete_data_sql)\n",
    "        # Execute the COPY command to insert new data\n",
    "        cursor_pgsql.copy_expert(sql=copy_sql, file=f)\n",
    "        # Execute UPDATE Time query to record the updated time of dimensional tables\n",
    "        cursor_pgsql.execute(f'UPDATE \"AR_HWM\".\"{table_name}\" SET \"UpdatedAt\" = CURRENT_TIMESTAMP;')\n",
    "\n",
    "    print(f\"{table_name} Data is inserted successfully into postgresql at {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dbd5f5-e3ed-428b-b5e6-b02ae9a78d1a",
   "metadata": {},
   "source": [
    "#### 3.2. Function - Insert Fact Table into PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae1ed3a6-c333-46f4-9aa6-cdce3cdda748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_fact_into_pgsql(table_name):\n",
    "    \"\"\" Insert fact data (AR) from csv file into PostgreSQL\"\"\"\n",
    "\n",
    "    # Create a temporary AR table to avoid duplicate rows insertion\n",
    "    tem_table_query = \"\"\"\n",
    "        DROP TABLE IF EXISTS \"temp_f_ar_hwm\";\n",
    "    \n",
    "        CREATE TEMPORARY TABLE \"temp_f_ar_hwm\" (\n",
    "        \"CustomerCode\" VARCHAR(50),\n",
    "        \"SellingPriceList\" VARCHAR(50),\n",
    "        \"SeriesName\" VARCHAR(50),\n",
    "        \"DocumentEntry\" BIGINT,\n",
    "        \"DocumentNumber\" BIGINT,\n",
    "        \"LineNum\" INT,\n",
    "        \"SaleEmployeeCode\" INT,\n",
    "        \"PostingDate\" TIMESTAMP,\n",
    "        \"DueDate\" TIMESTAMP,\n",
    "        \"DocumentDate\" TIMESTAMP,\n",
    "        \"InvoiceType\" VARCHAR(50),\n",
    "        \"ItemCode\" VARCHAR(50),\n",
    "        \"Quantity\" DECIMAL(10, 4),\n",
    "        \"Status\" VARCHAR(50),\n",
    "        \"Price\" DECIMAL(15, 4),  -- 15 digits total, 4 decimal places\n",
    "        \"LineTotal\" DECIMAL(20, 4),  -- 20 digits total, 4 decimal places\n",
    "        \"DocumentDiscount\" DECIMAL(15, 4),  -- 15 digits total, 4 decimal places\n",
    "        \"DiscountSum\" DECIMAL(15, 4),  -- 15 digits total, 4 decimal places\n",
    "        \"RowDiscountPercentage\" DECIMAL(7, 4),  -- 7 digits total, 4 decimal places\n",
    "        \"DocumentTotal\" DECIMAL(25, 4),  -- 25 digits total, 4 decimal places\n",
    "        \"WarehouseCode\" VARCHAR(50),\n",
    "        \"BPBranch\" VARCHAR(50),\n",
    "        \"SaleType\" VARCHAR(50),\n",
    "        \"Department\" VARCHAR(50),\n",
    "        \"AgentCode\" VARCHAR(50),\n",
    "        \"AgentCommissionAmount\" DECIMAL(20, 4),  -- 20 digits total, 4 decimal places\n",
    "        \"Project\" VARCHAR(50),\n",
    "        \"Currency\" VARCHAR(10),\n",
    "        \"GrossProfit\" DECIMAL(20, 4),  -- 20 digits total, 4 decimal places\n",
    "        \"LineDiscountTotal\" DECIMAL(20, 4),  -- 20 digits total, 4 decimal places\n",
    "        \"GrossTotal\" DECIMAL(25, 4),  -- 25 digits total, 4 decimal places\n",
    "        \"StockPrice\" DECIMAL(15, 4),  -- 15 digits total, 4 decimal places\n",
    "        \"StockValue\" DECIMAL(25, 4),  -- 25 digits total, 4 decimal places\n",
    "        \"ItemLastSalePrice\" DECIMAL(15, 4),  -- 15 digits total, 4 decimal places\n",
    "        \"GrossProfitItemCost\" DECIMAL(25, 4),  -- 25 digits total, 4 decimal places\n",
    "        \"TotalBasePriceLPP\" DECIMAL(25, 4),  -- 25 digits total, 4 decimal places\n",
    "        \"TotalGrossProfitLPP\" DECIMAL(25, 4),  -- 25 digits total, 4 decimal places\n",
    "        \"GLAccountCode\" BIGINT,\n",
    "        \"CogsAccountCode\" BIGINT,\n",
    "        \"Key\" VARCHAR(500)\n",
    "    );\"\"\"\n",
    "    \n",
    "      \n",
    "    # Copy data command to the temporay table\n",
    "    copy_to_tem_table_command = \"\"\"COPY \"temp_f_ar_hwm\" FROM stdin WITH CSV HEADER;\"\"\"\n",
    "\n",
    "\n",
    "    # Insert into main AR table\n",
    "    insert_to_main_f_table_query = f\"\"\"\n",
    "    INSERT INTO \"AR_HWM\".\"F_AR\" \n",
    "    SELECT\n",
    "    *\n",
    "    FROM \"temp_f_ar_hwm\"\n",
    "    ON CONFLICT(\"Key\") DO NOTHING;\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the CSV file\n",
    "    file_path = \"D:\\\\Data Warehousing\\\\AR HWM\\\\Fact\\\\\" + table_name + \".csv\"\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # Execute the temptable command\n",
    "        cursor_pgsql.execute(tem_table_query)\n",
    "        # Execute the COPY command\n",
    "        cursor_pgsql.copy_expert(sql=copy_to_tem_table_command, file=f)\n",
    "        # Execute the Insert command (main fact table)\n",
    "        cursor_pgsql.execute(insert_to_main_f_table_query)\n",
    "\n",
    "    print(f\"{table_name} Data is inserted successfully into postgresql at {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353fcd66-68d4-4ba5-ab68-056c8c31f730",
   "metadata": {},
   "source": [
    "#### 3.3. Create a composite key column to detect duplicate rows in AR ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2646c161-1bec-4a41-af30-82f54ea3e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(row):\n",
    "    return f\"{row['CustomerCode']} {row['SellingPriceList']} {row['SeriesName']} {row['Document Entry']} {row['Document Number']}{row['LineNum']}{row['ItemCode']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b1525c9-c965-4fe1-bc4e-07f63e5c5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_key(table_name):\n",
    "    filepath = f\"D:\\\\Data Warehousing\\\\AR HWM\\\\Fact\\\\{table_name}.csv\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['Key'] = df.apply(combine_columns, axis=1)\n",
    "    df.to_csv(f\"D:\\\\Data Warehousing\\\\AR HWM\\\\Fact\\\\{table_name}.csv\", index=False)\n",
    "    df.to_csv(f\"D:\\\\Data Warehousing\\\\AR HWM\\\\Fact\\\\{table_name}added_key.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6a6a3-73fa-4e46-bd3e-853e9f108272",
   "metadata": {},
   "source": [
    "#### 3.4. Load Dimension Tables and Fact Table into PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3581c052-e9d1-4414-9c9d-bf9fb0d69105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL Connection successful!\n",
      "D_Agent Data is inserted successfully into postgresql at 2024-06-06 10:28:41.903672\n",
      "D_Branch Data is inserted successfully into postgresql at 2024-06-06 10:28:42.065433\n",
      "D_COGSAccount Data is inserted successfully into postgresql at 2024-06-06 10:28:42.232642\n",
      "D_Customer Data is inserted successfully into postgresql at 2024-06-06 10:28:42.532351\n",
      "D_Department Data is inserted successfully into postgresql at 2024-06-06 10:28:42.703866\n",
      "D_GLAccount Data is inserted successfully into postgresql at 2024-06-06 10:28:42.864533\n",
      "D_Item Data is inserted successfully into postgresql at 2024-06-06 10:28:43.773757\n",
      "D_SaleType Data is inserted successfully into postgresql at 2024-06-06 10:28:43.964835\n",
      "D_Warehouse Data is inserted successfully into postgresql at 2024-06-06 10:28:44.126567\n",
      "All Dimensional Data are imported into PostgreSQL database successfully!\n",
      "F_AR_CM Data is inserted successfully into postgresql at 2024-06-06 10:28:44.509593\n",
      "Fact Data (Current Month) is imported.\n",
      "All Fact Data are imported into PostgreSQL database successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create pgsql connection and cursor\n",
    "conn_pgsql = connect_to_pgsql()\n",
    "cursor_pgsql = conn_pgsql.cursor()\n",
    "\n",
    "# Run the COPY command to import data from CSV\n",
    "try:\n",
    "    #Dim\n",
    "    for file in os.listdir(\"D:\\\\Data Warehousing\\\\AR HWM\\\\Dim\"):\n",
    "        table_name = file.split(\".\")[0]\n",
    "        insert_dim_into_pgsql(table_name)\n",
    "        # Commit the transaction\n",
    "        conn_pgsql.commit()\n",
    "    print(\"All Dimensional Data are imported into PostgreSQL database successfully!\")\n",
    "\n",
    "\n",
    "    # current date\n",
    "    current_date = datetime.now()\n",
    "    \n",
    "    #Fact\n",
    "    if current_date.day < 25:\n",
    "        # Delete AR Data WHERE PostingDate = Current Month\n",
    "        cursor_pgsql.execute(\"\"\"DELETE FROM \"AR_HWM\".\"F_AR\" WHERE \n",
    "        \"PostingDate\" = date_trunc('month', current_date)\"\"\")\n",
    "        \n",
    "        table_name = \"F_AR_CM\"\n",
    "        # Add Key column to detect duplicate rows\n",
    "        add_key(table_name)\n",
    "        insert_fact_into_pgsql(table_name)\n",
    "        # Commit the transaction\n",
    "        conn_pgsql.commit()\n",
    "        print(\"Fact Data (Current Month) is imported.\")\n",
    "        print(\"All Fact Data are imported into PostgreSQL database successfully!\")\n",
    "    else:\n",
    "        # Delete AR Data WHERE PostingDate=last month\n",
    "        cursor_pgsql.execute(\"\"\"DELETE FROM \"AR_HWM\".\"F_AR\" WHERE \n",
    "        \"PostingDate\" >= (date_trunc('month', current_date) - INTERVAL '1 month')\n",
    "        AND \"PostingDate\" < date_trunc('month', current_date)\"\"\")\n",
    "\n",
    "        # Load AR_LM into PgSQL\n",
    "        table_name = \"F_AR_LM\"\n",
    "        # Add Key column to detect duplicate rows\n",
    "        add_key(table_name)\n",
    "        insert_fact_into_pgsql(table_name)\n",
    "        # Commit the transaction\n",
    "        conn_pgsql.commit()\n",
    "\n",
    "        # Load AR_CM into PgSQL\n",
    "        table_name = \"F_AR_CM\"\n",
    "        # Add Key column to detect duplicate rows\n",
    "        add_key(table_name)\n",
    "        insert_fact_into_pgsql(table_name)\n",
    "        # Commit the transaction\n",
    "        conn_pgsql.commit()\n",
    "\n",
    "        print(\"Fact Data (Current Month and Last Month) are imported.\")\n",
    "        print(\"All Fact Data are imported into PostgreSQL database successfully!\")\n",
    "\n",
    "    \n",
    "except psycopg2.Error as e:\n",
    "    print(\"Error:\", e)\n",
    "finally:\n",
    "    # Close cursor and connection\n",
    "    cursor_pgsql.close()\n",
    "    conn_pgsql.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed892f5-bc19-4b0a-a563-0ac8520987cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
